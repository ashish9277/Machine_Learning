{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onager's Daily Tech Snippets\n",
      "None\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "hdr = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "      'Accept': 'text/html, application/xhtml+xml,application/xml;q=0.9,*/*q=0.8',\n",
    "       'Accept-Charset':'ISO-8859-1;utf-8,q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding':'none',\n",
    "       'Accept-Language':'en-US,en;q=0.8',\n",
    "       'Connection':'keep-alive'\n",
    "      }\n",
    "\n",
    "def getDoxyDonkeyText(url , token):\n",
    "        req = urllib2.Request(url, headers=hdr)\n",
    "        response = urllib2.urlopen(req)\n",
    "        page_text = response.read()\n",
    "        \n",
    "        soup = BeautifulSoup(page_text)\n",
    "        \n",
    "        title = soup.title.text\n",
    "        mydivs = soup.find_all('div' , {'class': token})\n",
    "        text = ' '.join(map(lambda p : p.text , mydivs))\n",
    "        \n",
    "        return text , title\n",
    "    \n",
    "\n",
    "def getAllDoxyDonkeyPosts(url, links):\n",
    "    req = urllib2.Request(url, headers=hdr)\n",
    "    response = urllib2.urlopen(req)\n",
    "    page_text = response.read()\n",
    "    \n",
    "    \n",
    "    #print page_text\n",
    "    soup = BeautifulSoup(page_text , \"lxml\")\n",
    "    print soup.title.text\n",
    "    for a in soup.find_all('a'):\n",
    "        try:\n",
    "            url = (a.get('href'))\n",
    "            print url\n",
    "            title = a.get('title')\n",
    "            if title == \"Older Posts\":\n",
    "                print(url)\n",
    "                links.append(url)\n",
    "                getAllDoxyDonkeyPosts(url,links)\n",
    "        except Exception as e:\n",
    "            print str(e)\n",
    "            print 'Error while parsing'\n",
    "            title = \"\"\n",
    "        return\n",
    "            \n",
    "blogUrl = 'https://doxydonkey.blogspot.com/'\n",
    "links = []\n",
    "getAllDoxyDonkeyPosts(blogUrl,links)\n",
    "doxyDonkeyPosts={}\n",
    "\n",
    "#print getDoxyDonkeyText(blogUrl , 'post-body')\n",
    "\n",
    "for link in links:\n",
    "    \n",
    "    doxyDonkeyPosts[url] = getDoxyDonkeyText(link , 'post-body')\n",
    "    \n",
    "documentCorpus =[]\n",
    "for onePost in doxyDonkeyPosts.values():\n",
    "    documentCorpus.append(onePost[0])\n",
    "    \n",
    "print documentCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=2,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "vectorize = TfidfVectorizer(max_df = 0.5 , min_df= 2, stop_words= 'english')\n",
    "print vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0208753584ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocumentCorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;31m#km = KMeans(n_cluster = 5, init ='k-means++' , max_iter =100 , n_init = 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ashish Agrawal\\Anaconda2\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \"\"\"\n\u001b[0;32m-> 1305\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ashish Agrawal\\Anaconda2\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ashish Agrawal\\Anaconda2\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    765\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "X= vectorize.fit_transform(documentCorpus)\n",
    "#km = KMeans(n_cluster = 5, init ='k-means++' , max_iter =100 , n_init = 1)\n",
    "print X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\n\\nAlthough the use of internet and digital materials in the language classroom has come a long way over the last 20 years, still the vast majority of web based material that finds its way into the language classroom is used for information input or comprehension purposes. The students\\u2019 interaction with the materials is as such largely passive with the teacher controlling the suitability of the materials selected and deciding what information the students will extract from it.\\n\\n\\n\\n\\nIn Thinking Critically through Digital Media I have tried to build on this model, but develop it and take it to deeper and more critical levels of analysis that go beyond the superficial linguistic level and help to develop students not only as English language speakers, but as capable information literate participants in the global knowledge economy.\\n\\nThe book uses as its basis the development of key digital literacies. These include the ability to understand visually presented data, the ability collect and analyse data using a range of techniques and survey tools and the ability to create and deliver a range of presentation types using digital media tools.\\n\\n\\nWhilst developing these digital literacies students are also encouraged to assess the validity, credibility and underlying bias of the information they study and are given a range of research tools and techniques for reassessing the information and evaluating how it fits within their personal framework of belief systems and values.\\n\\n\\nThe book itself has four main chapters. The first three chapters contain a range of activities that teachers can use with students to develop their abilities to understand and create infographics, develop research polls and surveys and create and deliver presentations. These activities give students hands on exposure to a range of recommended tools and develop students as active creators of information whilst developing their abilities to work collaboratively in digital online environments.\\n\\n\\n\\n\\nThe fourth key chapter of the book is a collection of lesson plans that teachers can use to take students through a complete process from accessing their existing knowledge about a topic, understanding new input, examining how the information fits into their existing value scheme, checking the credibility and validity of the information, carrying out their own parallel research through social media to finally sharing and reevaluating what they have learned.\\n\\n\\n\\n\\nI believe that the skills and abilities teachers can help students develop through the use of these materials are ones that are sadly lacking, not only in the English language classroom but also in the general education of many students around the world. Through the use of these materials I hope teachers can develop more actively and intellectually critical students who approach digital media with the ability not only to comprehend and consume information but also understand the possible bias, motivation and underlying values of those creating the information. I believe these skills and abilities are key to creating a more tolerant, open-minded and critically aware global society.\\n\\nRelated links:\\n\\nThinking Critically through Digital Media\\xa0\\nPeacheyPublications\\xa0\\nTools and alternatives for creating presentations\\nTechnology, Autonomous Learning & a Decline in Critical Thinking\\n\\n\\nBest\\nNik Peachey\\n\\n\\n\\n\\n', u\"Nik's Learning Technology Blog\")\n"
     ]
    }
   ],
   "source": [
    "print getDoxyDonkeyText('https://nikpeachey.blogspot.com/' , 'post-body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
